{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import itertools\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "from src.util import load_samples\n",
    "from src.data.data_collator import LegalDataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST2025_EN_PATH = \"data/COLIEE2025statute_data-English/train/R05_en.xml\"\n",
    "\n",
    "SELECTED_ID = \"R05\"\n",
    "\n",
    "RAW_DATA_DIR = \"data\"\n",
    "DATA_OUTPUT_DIR = \"data/synthesys\"\n",
    "QUERY_PATH = os.path.join(RAW_DATA_DIR, \"COLIEE2025statute_data-English/train\")\n",
    "ARTICLE_PATH = os.path.join(RAW_DATA_DIR, \"full_en_civil_code_df_24.csv\")\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints_kg\"\n",
    "STEP1_CHECKPOINT_DIR = f\"{CHECKPOINT_DIR}/step1_bge_pre_retrieval\"\n",
    "STEP2_CHECKPOINT_DIR = f\"{CHECKPOINT_DIR}/step2_rankllama_retrieval\"\n",
    "STEP3_CHECKPOINT_DIR = f\"{CHECKPOINT_DIR}/step3_final_retrieval\"\n",
    "\n",
    "ACCEPTED_MODELS = [\n",
    "    \"e5_mistral_7b_instruct\",\n",
    "    \"gemma_2_9b_it\",\n",
    "    \"gemma_2_27b_it\",\n",
    "    \"phi_3_medium_4k_instruct\",\n",
    "]\n",
    "\n",
    "\n",
    "# TODO: fix bug\n",
    "BUG_ARTICLE_POSTFIX = \"(1)\"  # In the R04's task 3 label, there are some ground truth labels having \"(1)\" postfix. We need to remove them.\n",
    "\n",
    "\n",
    "INFERENCE_DIR = f\"{CHECKPOINT_DIR}/inference\"\n",
    "\n",
    "\n",
    "# Step 1\n",
    "BGE_TOP = 100\n",
    "BGE_SEQUENCE_MAX_LENGTH = 1024\n",
    "HISTOGRAM_N_POSITIVE_REPLICATES = 300\n",
    "\n",
    "\n",
    "# Step 2\n",
    "RANKLLAMA_MAX_LENGTH = 1024\n",
    "RANKLLAMA_THRESHOLD = -3.5 # preserve about 50 candidates for each query\n",
    "RANKLLAMA_TOP = 50\n",
    "\n",
    "\n",
    "# Step 4\n",
    "CUT_OFF_THRESHOLD = 0.3687529996711829\n",
    "WEIGHTS = np.array([0.23716786, 0.21487627, 0.3068145 , 0.24114137])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data\n",
    "# Load the article data\n",
    "en_article_df = pd.read_csv(ARTICLE_PATH)\n",
    "en_article_df.rename(columns={\"article\": \"article_id\", \"content\": \"article_content\"}, inplace=True)\n",
    "\n",
    "# Load the query data\n",
    "query_files = glob.glob(f\"{QUERY_PATH}/*.xml\")\n",
    "\n",
    "queries = []\n",
    "for query_file in query_files:\n",
    "    queries += load_samples(query_file)\n",
    "\n",
    "en_query_df = pd.DataFrame(queries)\n",
    "en_query_df = en_query_df.rename(columns={\"index\": \"query_id\",\n",
    "                                          \"content\": \"query_content\",\n",
    "                                          \"result\": \"task3_label\",\n",
    "                                          \"label\": \"task4_label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_df = en_query_df[en_query_df[\"query_id\"].str.startswith(SELECTED_ID)].copy(deep=True)\n",
    "\n",
    "del en_query_df\n",
    "\n",
    "if len(test_query_df) == 0:\n",
    "    queries = load_samples(TEST2025_EN_PATH)\n",
    "\n",
    "    test_query_df = pd.DataFrame(queries)\n",
    "    test_query_df = test_query_df.rename(columns={\"index\": \"query_id\",\n",
    "                                                    \"content\": \"query_content\",\n",
    "                                                    \"result\": \"task3_label\",\n",
    "                                                    \"label\": \"task4_label\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"task3_label\" in test_query_df.columns:\n",
    "    test_query_df = test_query_df.drop(columns=[\"task3_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: BGE Pre-retrieval\n",
    "\n",
    "### 1.1. BGE Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, device='cuda')\n",
    "\n",
    "# article embedding\n",
    "article_embeddings = model.encode(en_article_df[\"article_content\"].tolist(),\n",
    "                                  batch_size=32,\n",
    "                                  max_length=BGE_SEQUENCE_MAX_LENGTH\n",
    "                                  )['dense_vecs']\n",
    "\n",
    "\n",
    "# query embedding\n",
    "query_embeddings = model.encode(test_query_df[\"query_content\"].tolist(),\n",
    "                                batch_size=32,\n",
    "                                max_length=BGE_SEQUENCE_MAX_LENGTH\n",
    "                                )['dense_vecs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "###LOAD####\n",
    "article_embeddings = joblib.load(f\"./{STEP1_CHECKPOINT_DIR}/article_embeddings.pkl\")\n",
    "query_embeddings = joblib.load(f\"./{STEP1_CHECKPOINT_DIR}/query_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embedding_dict = dict(zip(en_article_df[\"article_id\"].tolist(), article_embeddings))\n",
    "query_embedding_dict = dict(zip(test_query_df[\"query_id\"].tolist(), query_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Retrieval with Histogram-based Gradient Boosting\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(query_id, labels):\n",
    "    return list(itertools.product([query_id], labels))\n",
    "\n",
    "\n",
    "def distance_function(query_emb, article_emb):\n",
    "    return query_emb - article_emb\n",
    "\n",
    "\n",
    "def get_distance(query_id, article_id, query_embedding_dict, article_embedding_dict):\n",
    "    query_emb = query_embedding_dict[query_id]\n",
    "    article_emb = article_embedding_dict[article_id]\n",
    "\n",
    "    return distance_function(query_emb, article_emb)\n",
    "\n",
    "\n",
    "query_article_pairs = test_query_df.apply(lambda x: make_pairs(x[\"query_id\"], en_article_df[\"article_id\"].values), axis=1)\n",
    "query_article_pairs = sum(query_article_pairs, [])\n",
    "\n",
    "X_test = list(map(lambda x: get_distance(*x, query_embedding_dict, article_embedding_dict), query_article_pairs))\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_preds(group):\n",
    "    group = group.sort_values(\"step1_score\", ascending=False)\n",
    "\n",
    "    # cut_off_score = group.iloc[BGE_TOP][\"step1_score\"]\n",
    "    # group[\"keep\"] = group[\"step1_score\"] > cut_off_score - 1e-5\n",
    "\n",
    "    group[\"keep\"] = False\n",
    "    group.iloc[:BGE_TOP, group.columns.get_loc(\"keep\")] = True\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "model = joblib.load(open(f\"{STEP1_CHECKPOINT_DIR}/histogram_classifier.pkl\", \"rb\"))\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "test_df_step1 = pd.DataFrame(query_article_pairs, columns=[\"query_id\", \"article_id\"])\n",
    "test_df_step1[\"step1_score\"] = y_pred[:, 1]\n",
    "\n",
    "test_df_step1 = test_df_step1.groupby(\"query_id\")[test_df_step1.columns.tolist()]\\\n",
    "                             .apply(get_top_preds)\\\n",
    "                             .reset_index(drop=True)\n",
    "\n",
    "test_df_step1 = test_df_step1[test_df_step1[\"keep\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: RankLlama for 2nd-stage retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\n",
    "                                                                    num_labels=1,\n",
    "                                                                    torch_dtype=torch.bfloat16,\n",
    "                                                                    device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_rankllama_prompt(row):\n",
    "    \"\"\"\n",
    "    Build one of two prompts:\n",
    "    - Prompt 1 (original candidate)\n",
    "    - Prompt 2 (KG-connected candidate with relation + original relevance)\n",
    "    Expects row to contain at least:\n",
    "      - row['query_content'] or fallback ''\n",
    "      - row['article_content'] or fallback ''\n",
    "      - row.get('is_kg_added', False)\n",
    "      - row.get('relation') for KG edges (optional)\n",
    "      - row.get('source_relevance_label') or row.get('relevance_label') for original relevance (optional)\n",
    "    \"\"\"\n",
    "    query_text = str(row.get(\"query_content\", \"\") or \"\")\n",
    "    article_text = str(row.get(\"article_content\", \"\") or \"\")\n",
    "\n",
    "    # normalize relation and relevance inputs\n",
    "    relation_type = str(row.get(\"relation\") or row.get(\"relation_type\") or \"N/A\")\n",
    "    # prefer explicit source_relevance_label (e.g., from parent), else fall back to any relevance_label\n",
    "    relevance_level = row.get(\"source_relevance_label\") or row.get(\"relevance_label\") or row.get(\"source_relevance\") or \"Unknown\"\n",
    "    relevance_level = str(relevance_level)\n",
    "\n",
    "    # Prompt 1 (original article)\n",
    "    prompt1 = f\"\"\"You are a legal retrieval assistant. Given a query and\n",
    "a legal article, determine how relevant the article is\n",
    "to answering the query.\n",
    "Query: {query_text}\n",
    "Article: {article_text}\n",
    "Please provide a relevance score.\"\"\"\n",
    "    # Prompt 2 (KG-connected article)\n",
    "    prompt2 = f\"\"\"You are a legal retrieval assistant .Given a query and\n",
    "a legal article connected through a legal knowledge\n",
    "graph , determine its relevance to the query.\n",
    "Query:{query_text}\n",
    "Article:{article_text}\n",
    "Relationtooriginalarticle:{relation_type}\n",
    "Original article relevance:{relevance_level}\n",
    "Please provide a relevance score.\"\"\"\n",
    "\n",
    "    return prompt2 if bool(row.get(\"is_kg_added\", False)) else prompt1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch  # ensure torch is available for dtype/device handling\n",
    "\n",
    "def get_scores(model, tokenizer, df, batch_size, max_len, data_collator, debug_dtype=False):\n",
    "    \"\"\"\n",
    "    Robust get_scores: converts logits to float32 before moving to numpy to avoid\n",
    "    errors when logits are in bfloat16/float16.\n",
    "    Uses dynamic prompting via make_rankllama_prompt(row) which should inspect\n",
    "    row['is_kg_added'], relation, source_article_id, source_step1_score, etc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    scores = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"scoring\"):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "\n",
    "        # Build prompts using dynamic prompt factory which uses KG metadata when present\n",
    "        # make_rankllama_prompt(row) must return a single string prompt for that row\n",
    "        texts = batch.apply(lambda r: make_rankllama_prompt(r), axis=1).tolist()\n",
    "\n",
    "        # Tokenize -> list-of-dicts (keeps your existing collator flow)\n",
    "        tokenized = tokenizer(texts, max_length=max_len, truncation=True)\n",
    "        inputs_list = [dict(zip(tokenized.keys(), vals)) for vals in zip(*tokenized.values())]\n",
    "\n",
    "        collated = data_collator(inputs_list)\n",
    "        collated = {k: v.to(device) for k, v in collated.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**collated)\n",
    "\n",
    "        logits = getattr(outputs, \"logits\", None)\n",
    "\n",
    "        # fallback default scores\n",
    "        if logits is None:\n",
    "            batch_scores = [0.0] * len(texts)\n",
    "        else:\n",
    "            # Ensure logits are on CPU and in float32 before converting to numpy\n",
    "            # (handles bfloat16/float16 returned by some model setups)\n",
    "            if isinstance(logits, torch.Tensor):\n",
    "                # move to cpu and cast to float32\n",
    "                logits_cpu = logits.detach().cpu().to(torch.float32)\n",
    "\n",
    "                if debug_dtype:\n",
    "                    print(\"logits original dtype:\", logits.dtype, \"-> converted dtype:\", logits_cpu.dtype)\n",
    "\n",
    "                if logits_cpu.dim() == 2:\n",
    "                    if logits_cpu.size(1) == 1:\n",
    "                        batch_scores = logits_cpu[:, 0].numpy()\n",
    "                    elif logits_cpu.size(1) == 2:\n",
    "                        probs = torch.softmax(logits_cpu, dim=1)[:, 1]\n",
    "                        batch_scores = probs.numpy()\n",
    "                    else:\n",
    "                        probs = torch.softmax(logits_cpu, dim=1)[:, 1] if logits_cpu.size(1) > 1 else logits_cpu[:, 0]\n",
    "                        batch_scores = probs.numpy()\n",
    "                elif logits_cpu.dim() == 1:\n",
    "                    batch_scores = logits_cpu.numpy()\n",
    "                else:\n",
    "                    # fallback for token logits etc.\n",
    "                    batch_scores = logits_cpu.view(logits_cpu.size(0), -1)[:, 0].numpy()\n",
    "            else:\n",
    "                # non-tensor logits (rare)\n",
    "                batch_scores = np.array(logits).astype(np.float32).squeeze()\n",
    "\n",
    "        # ensure python floats and length matches batch\n",
    "        batch_scores_arr = np.asarray(batch_scores).squeeze()\n",
    "        # if single scalar returned for batch, replicate to batch size\n",
    "        if batch_scores_arr.shape == ():\n",
    "            batch_scores_arr = np.repeat(float(batch_scores_arr), len(texts))\n",
    "        # If lengths mismatch, pad/truncate conservatively\n",
    "        if len(batch_scores_arr) != len(texts):\n",
    "            if len(batch_scores_arr) < len(texts):\n",
    "                batch_scores_arr = np.concatenate([batch_scores_arr, np.zeros(len(texts) - len(batch_scores_arr), dtype=np.float32)])\n",
    "            else:\n",
    "                batch_scores_arr = batch_scores_arr[:len(texts)]\n",
    "\n",
    "        scores.extend([float(x) for x in batch_scores_arr])\n",
    "\n",
    "    return np.array(scores).squeeze().tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_preds(group):\n",
    "    group = group.sort_values(\"step2_score\", ascending=False)\n",
    "\n",
    "    cut_off_score = group.iloc[RANKLLAMA_TOP][\"step2_score\"]\n",
    "    group[\"keep\"] = group[\"step2_score\"] > cut_off_score - 1e-5\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "model = get_model('castorini/rankllama-v1-7b-lora-passage')\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "tokenizer.pad_token = \"<unk>\"\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kg.phase1kg.db_connection import Neo4jConnection\n",
    "\n",
    "conn = Neo4jConnection(\"neo4j:// 10.50.22.71:7687\", \"neo4j\", \"Smitmaurya@24\")\n",
    "result = conn.execute_query(\"RETURN 'Connected to Neo4j!' AS message\")\n",
    "for record in result:\n",
    "    print(record[\"message\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires: neo4j, pandas, numpy, tqdm\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "NODE_ID_PROP = \"number\"  # change if your Article node uses a different id prop\n",
    "driver = conn.driver  # your Neo4j connection driver\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def _parse_vector(v):\n",
    "    \"\"\"Safely parse a Neo4j-stored vector which may be a list, numpy array or JSON/string.\"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return np.asarray(v, dtype=np.float32)\n",
    "    if isinstance(v, str):\n",
    "        try:\n",
    "            # try json first\n",
    "            parsed = json.loads(v)\n",
    "            return np.asarray(parsed, dtype=np.float32)\n",
    "        except Exception:\n",
    "            try:\n",
    "                # fallback to eval (last resort)\n",
    "                parsed = eval(v)\n",
    "                return np.asarray(parsed, dtype=np.float32)\n",
    "            except Exception:\n",
    "                return None\n",
    "    # any other type\n",
    "    try:\n",
    "        return np.asarray(v, dtype=np.float32)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"Robust cosine similarity (returns 0.0 if any vector is invalid).\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return 0.0\n",
    "    a = np.asarray(a, dtype=np.float32)\n",
    "    b = np.asarray(b, dtype=np.float32)\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return 0.0\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def assign_relevance_labels_by_rank(df, score_col=\"step2_score\"):\n",
    "    \"\"\"\n",
    "    For each query, label by rank:\n",
    "      - top 30 -> 'Highly Relevant'\n",
    "      - next 30 -> 'Moderately Relevant'\n",
    "      - rest -> 'Less Relevant'\n",
    "    Uses score_col; if absent falls back to 'step1_score'.\n",
    "    \"\"\"\n",
    "    sc = score_col if score_col in df.columns else \"step1_score\"\n",
    "    def _label(group):\n",
    "        group = group.sort_values(sc, ascending=False).reset_index(drop=True)\n",
    "        group[\"relevance_label\"] = \"Less Relevant\"\n",
    "        # top 30 -> Highly\n",
    "        group.loc[0:29, \"relevance_label\"] = \"Highly Relevant\"\n",
    "        # next 30 -> Moderately (30..59)\n",
    "        if len(group) > 30:\n",
    "            group.loc[30:59, \"relevance_label\"] = \"Moderately Relevant\"\n",
    "        return group\n",
    "    if df.shape[0] == 0:\n",
    "        return df\n",
    "    labeled = df.groupby(\"query_id\", as_index=False, group_keys=False).apply(_label)\n",
    "    return labeled.reset_index(drop=True)\n",
    "\n",
    "# ----------------- candidate_expansion with vector gating -----------------\n",
    "def candidate_expansion(temp_df,\n",
    "                        en_article_df: pd.DataFrame,\n",
    "                        en_query_df: pd.DataFrame,\n",
    "                        max_candidates: int = 150,\n",
    "                        parent_limit: int = 30,\n",
    "                        batch_size: int = 16,\n",
    "                        bge_similarity_threshold: float = 0.6):\n",
    "    \"\"\"\n",
    "    Expand temp_df (top-K per query) by traversing parents top->bottom (only first `parent_limit`\n",
    "    parents) and adding 1-hop Article neighbors via REFERENCES|REFERENCED_BY **only if**\n",
    "    cosine_sim(source_node.vector, neighbor.node.vector) > bge_similarity_threshold.\n",
    "\n",
    "    Also assigns relevance labels at the end (prefers step2_score if present).\n",
    "    \"\"\"\n",
    "    df = temp_df.copy(deep=True).reset_index(drop=True)\n",
    "    df[\"step1_score\"] = df[\"step1_score\"].astype(float)\n",
    "\n",
    "    # Build gold-map: query_id -> set(positive_article_ids)\n",
    "    gold_map = {}\n",
    "    if en_query_df is not None and \"query_id\" in en_query_df.columns and \"task3_label\" in en_query_df.columns:\n",
    "        for _, r in en_query_df.iterrows():\n",
    "            qid = r[\"query_id\"]\n",
    "            vals = r[\"task3_label\"]\n",
    "            if vals is None or (isinstance(vals, float) and np.isnan(vals)):\n",
    "                gold_map[qid] = set()\n",
    "            elif isinstance(vals, (list, np.ndarray, pd.Series)):\n",
    "                gold_map[qid] = set([str(x) for x in vals])\n",
    "            elif isinstance(vals, str):\n",
    "                try:\n",
    "                    parsed = eval(vals)\n",
    "                    if isinstance(parsed, (list, tuple, set, np.ndarray)):\n",
    "                        gold_map[qid] = set([str(x) for x in parsed])\n",
    "                    else:\n",
    "                        gold_map[qid] = set([str(parsed)])\n",
    "                except Exception:\n",
    "                    parts = [v.strip() for v in vals.strip(\"[] \").split(\",\") if v.strip()]\n",
    "                    gold_map[qid] = set(parts)\n",
    "            else:\n",
    "                gold_map[qid] = set([str(vals)])\n",
    "    else:\n",
    "        gold_map = {}\n",
    "\n",
    "    # Prepare article content/label maps if available\n",
    "    article_content_map = {}\n",
    "    if en_article_df is None:\n",
    "        en_article_df = globals().get(\"en_article_df\", None)\n",
    "    if en_article_df is not None:\n",
    "        if \"article_id\" in en_article_df.columns and \"article_content\" in en_article_df.columns:\n",
    "            article_content_map = dict(zip(en_article_df[\"article_id\"].astype(str), en_article_df[\"article_content\"]))\n",
    "\n",
    "    # Build initial grouped dict from provided top-K\n",
    "    grouped = {}\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"query_id\"]\n",
    "        aid = str(row[\"article_id\"])\n",
    "        grouped.setdefault(q, {})\n",
    "        grouped[q].setdefault(aid, {\n",
    "            \"query_id\": q,\n",
    "            \"article_id\": aid,\n",
    "            \"step1_score\": float(row[\"step1_score\"]),\n",
    "            \"label\": row.get(\"label\", 0.0),\n",
    "            \"keep\": row.get(\"keep\", True),\n",
    "            \"query_content\": row.get(\"query_content\", None),\n",
    "            \"article_content\": row.get(\"article_content\", article_content_map.get(aid, None)),\n",
    "            \"parent_node\": None,\n",
    "            \"relation\": None\n",
    "        })\n",
    "\n",
    "    # neighbor cache: src_id -> list of dicts {nid, rel, nvec (np.array), sim (float)}\n",
    "    neighbor_cache = {}\n",
    "\n",
    "    def fetch_neighbors_batch(article_ids):\n",
    "        \"\"\"\n",
    "        Batch fetch neighbors with relation types and node.vector for both source and neighbors.\n",
    "        Stores neighbor_cache[src] = list of dicts {nid, rel, nvec, sim} where sim is computed\n",
    "        between src_vector and nvec (if both available).\n",
    "        Returns dict src->list((nid, rel, nvec, sim)).\n",
    "        \"\"\"\n",
    "        to_query = [aid for aid in article_ids if str(aid) not in neighbor_cache]\n",
    "        out = {str(aid): [] for aid in article_ids}\n",
    "        if not to_query:\n",
    "            for aid in article_ids:\n",
    "                out[str(aid)] = neighbor_cache.get(str(aid), [])\n",
    "            return out\n",
    "        if driver is None:\n",
    "            for aid in article_ids:\n",
    "                out[str(aid)] = neighbor_cache.get(str(aid), [])\n",
    "            return out\n",
    "\n",
    "        # Cypher: return src vector and neighbors with their vectors\n",
    "        cypher = f\"\"\"\n",
    "        UNWIND $aids AS aid\n",
    "        MATCH (a:Article {{{NODE_ID_PROP}: aid}})\n",
    "        OPTIONAL MATCH (a)-[r:REFERENCES|REFERENCED_BY]-(n:Article)\n",
    "        RETURN aid AS src, a.vector AS src_vector,\n",
    "               collect(DISTINCT {{id: n.{NODE_ID_PROP}, rel: type(r), nvec: n.vector}}) AS neighbors\n",
    "        \"\"\"\n",
    "        for i in range(0, len(to_query), batch_size):\n",
    "            chunk = to_query[i:i+batch_size]\n",
    "            params_chunk = [int(x) if NODE_ID_PROP == \"number\" and str(x).isdigit() else x for x in chunk]\n",
    "            with driver.session() as session:\n",
    "                res = session.run(cypher, aids=params_chunk)\n",
    "                for r in res:\n",
    "                    src = str(r[\"src\"])\n",
    "                    src_vec_raw = r.get(\"src_vector\", None)\n",
    "                    src_vec = _parse_vector(src_vec_raw)\n",
    "                    neighs = r.get(\"neighbors\", []) or []\n",
    "                    parsed = []\n",
    "                    for item in neighs:\n",
    "                        nid = item.get(\"id\")\n",
    "                        if nid is None:\n",
    "                            continue\n",
    "                        nid = str(nid)\n",
    "                        rel = item.get(\"rel\", None)\n",
    "                        nvec_raw = item.get(\"nvec\", None)\n",
    "                        nvec = _parse_vector(nvec_raw)\n",
    "                        sim = cosine_sim(src_vec, nvec) if (src_vec is not None and nvec is not None) else 0.0\n",
    "                        parsed.append({\"nid\": nid, \"rel\": rel, \"nvec\": nvec, \"sim\": sim})\n",
    "                    neighbor_cache[src] = parsed\n",
    "\n",
    "        for aid in article_ids:\n",
    "            out[str(aid)] = neighbor_cache.get(str(aid), [])\n",
    "        return out\n",
    "\n",
    "    # Iterate queries with progress; parents limited to parent_limit\n",
    "    for qid in tqdm(list(grouped.keys()), desc=\"queries\"):\n",
    "        rows = grouped[qid]\n",
    "        parents_sorted = sorted(rows.values(), key=lambda x: -float(x[\"step1_score\"]))\n",
    "        parents_limited = parents_sorted[:parent_limit]\n",
    "        cand_count = len(rows)\n",
    "\n",
    "        parent_ids = [p[\"article_id\"] for p in parents_limited]\n",
    "\n",
    "        for pid in tqdm(parent_ids, desc=f\"parents for {qid}\", leave=False):\n",
    "            if cand_count >= max_candidates:\n",
    "                break\n",
    "            parent_row = rows[pid]\n",
    "            parent_score = float(parent_row[\"step1_score\"])\n",
    "\n",
    "            neigh_map = fetch_neighbors_batch([pid])\n",
    "            neighbors = neigh_map.get(str(pid), [])\n",
    "\n",
    "            for nmeta in neighbors:\n",
    "                if cand_count >= max_candidates:\n",
    "                    break\n",
    "                n_id = str(nmeta[\"nid\"])\n",
    "                rel = nmeta.get(\"rel\")\n",
    "                sim_src_neighbor = float(nmeta.get(\"sim\", 0.0))\n",
    "\n",
    "                # Skip self and require similarity threshold\n",
    "                if n_id == str(pid):\n",
    "                    continue\n",
    "                if sim_src_neighbor <= bge_similarity_threshold:\n",
    "                    # do NOT add or update neighbor if similarity not above threshold\n",
    "                    continue\n",
    "\n",
    "                # If neighbor not present, add it\n",
    "                if n_id not in rows:\n",
    "                    gold_set = gold_map.get(qid, set())\n",
    "                    neighbor_label = 1.0 if n_id in gold_set else 0.0\n",
    "                    rows[n_id] = {\n",
    "                        \"query_id\": qid,\n",
    "                        \"article_id\": n_id,\n",
    "                        \"step1_score\": parent_score,   # inherit parent's score\n",
    "                        \"label\": neighbor_label,\n",
    "                        \"keep\": True,\n",
    "                        \"query_content\": parent_row.get(\"query_content\"),\n",
    "                        \"article_content\": article_content_map.get(n_id, None),\n",
    "                        \"parent_node\": pid,\n",
    "                        \"relation\": rel,\n",
    "                        # store similarity and indication that it was KG-added\n",
    "                        \"source_sim\": sim_src_neighbor,\n",
    "                        \"is_kg_added\": True\n",
    "                    }\n",
    "                    cand_count += 1\n",
    "                else:\n",
    "                    # already present: update only if parent's score is higher AND sim > threshold\n",
    "                    existing_score = float(rows[n_id][\"step1_score\"])\n",
    "                    if parent_score > existing_score and sim_src_neighbor > bge_similarity_threshold:\n",
    "                        rows[n_id][\"step1_score\"] = parent_score\n",
    "                        rows[n_id][\"parent_node\"] = pid\n",
    "                        rows[n_id][\"relation\"] = rel\n",
    "                        rows[n_id][\"source_sim\"] = sim_src_neighbor\n",
    "                        rows[n_id][\"is_kg_added\"] = True\n",
    "                        # keep gold label if present\n",
    "                        if n_id in gold_map.get(qid, set()):\n",
    "                            rows[n_id][\"label\"] = 1.0\n",
    "                        # else preserve existing label if any\n",
    "\n",
    "    # flatten grouped into DataFrame\n",
    "    expanded_rows = []\n",
    "    for q, amap in grouped.items():\n",
    "        for aid, vals in amap.items():\n",
    "            expanded_rows.append(vals)\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    # ensure expected columns present\n",
    "    expected_cols = [\"query_id\", \"article_id\", \"step1_score\", \"label\", \"keep\",\n",
    "                     \"query_content\", \"article_content\", \"parent_node\", \"relation\",\n",
    "                     \"is_kg_added\", \"source_sim\"]\n",
    "    for col in expected_cols:\n",
    "        if col not in expanded_df.columns:\n",
    "            expanded_df[col] = None\n",
    "\n",
    "    # sort within each query by descending step1_score\n",
    "    expanded_df = expanded_df.groupby(\"query_id\", sort=False, group_keys=False).apply(\n",
    "        lambda g: g.sort_values(\"step1_score\", ascending=False)\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Assign relevance labels using step2_score if it exists (RankLLaMA output),\n",
    "    # otherwise fallback to step1_score.\n",
    "    score_col_to_use = \"step2_score\" if \"step2_score\" in expanded_df.columns else \"step1_score\"\n",
    "    expanded_df = assign_relevance_labels_by_rank(expanded_df, score_col=score_col_to_use)\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = LegalDataCollatorWithPadding(tokenizer)\n",
    "\n",
    "test_df_step2 = test_df_step1.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2=candidate_expansion(test_df_step2,en_article_df,test_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expanded_test_df_step2 = expanded_test_df_step2.merge(test_query_df[[\"query_id\", \"query_content\"]], how=\"left\")\n",
    "expanded_test_df_step2 = expanded_test_df_step2.merge(en_article_df[[\"article_id\", \"article_content\"]], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step2_scores = get_scores(model, tokenizer, expanded_test_df_step2, 16, RANKLLAMA_MAX_LENGTH, data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2[\"step2_score\"] = test_step2_scores\n",
    "expanded_test_df_step2 = expanded_test_df_step2.groupby(\"query_id\")[expanded_test_df_step2.columns.tolist()]\\\n",
    "                             .apply(get_top_preds)\\\n",
    "                             .reset_index(drop=True)\n",
    "\n",
    "expanded_test_df_step2 = expanded_test_df_step2[expanded_test_df_step2[\"keep\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2.to_json(f\"{CHECKPOINT_DIR}/inference/{SELECTED_ID}_step2.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to run.sh to get the predicted logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_test_df_step2 = pd.read_json(f\"{CHECKPOINT_DIR}/inference/{SELECTED_ID}_step2.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_logits(file_paths):\n",
    "    logits = []\n",
    "    for file_path in file_paths:\n",
    "        preds = np.load(file_path)\n",
    "        preds = softmax(preds, axis=1)\n",
    "\n",
    "        logits.append(preds[:, 1])\n",
    "\n",
    "    return np.array(logits).T\n",
    "\n",
    "\n",
    "all_logit_path = [INFERENCE_DIR + f\"/{SELECTED_ID}eval/\" + model + f\"/{SELECTED_ID}_step2_logits.npy\" for model in ACCEPTED_MODELS]\n",
    "all_logit_path.sort()\n",
    "\n",
    "all_logits = load_logits(all_logit_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: review the top filter\n",
    "def top_filter(group_df):\n",
    "    group_df = group_df.sort_values(by=[\"step3_score\"],\n",
    "                                          ascending=False,\n",
    "                                          ignore_index=True)\n",
    "    return group_df[:2]\n",
    "\n",
    "\n",
    "def fill_none_predicted(row, step3_top2_df):\n",
    "    if type(row[\"article_id\"]) == list:\n",
    "        return row\n",
    "    row[\"article_id\"] = step3_top2_df[step3_top2_df[\"query_id\"] == row[\"query_id\"]][\"article_id\"].values[0]\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (np.dot(all_logits, WEIGHTS) > CUT_OFF_THRESHOLD).astype(int)\n",
    "\n",
    "test_df_step3 = expanded_test_df_step2.copy(deep=True)\n",
    "test_df_step3[\"keep\"] = preds & (test_df_step3[\"step2_score\"] > RANKLLAMA_THRESHOLD)\n",
    "test_df_step3 = test_df_step3[test_df_step3[\"keep\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_step3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step3_score_df = expanded_test_df_step2.copy(deep=True)\n",
    "step3_score_df[\"step3_score\"] = np.dot(all_logits, WEIGHTS)\n",
    "\n",
    "step3_top2_df = step3_score_df.drop_duplicates(subset=[\"query_id\", \"article_id\"])\\\n",
    "    .groupby(\"query_id\")[step3_score_df.columns]\\\n",
    "    .apply(top_filter)\\\n",
    "    .reset_index(drop=True)\n",
    "step3_top2_df = step3_top2_df.groupby(\"query_id\")[\"article_id\"].apply(list).reset_index()\n",
    "\n",
    "\n",
    "submission_df = test_df_step3.copy(deep=True)\n",
    "submission_df = submission_df.groupby(\"query_id\")[\"article_id\"].apply(list).reset_index()\n",
    "submission_df = submission_df.merge(test_query_df, on=\"query_id\", how=\"right\")\n",
    "\n",
    "\n",
    "# In some cases, we can't find any predicted articles. We need to fill them with the top 2 articles from step 3\n",
    "submission_df = submission_df.apply(lambda x: fill_none_predicted(x, step3_top2_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_json(f\"./{CHECKPOINT_DIR}/final/{SELECTED_ID}_submission.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# Config: choose max k to evaluate precision@k / recall@k\n",
    "# If None, will use the maximal prediction length in your dataframe.\n",
    "MAX_K = None   # or set e.g. 5 or 10\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "def precision_at_k_single(preds, gold_set, k):\n",
    "    \"\"\"Standard precision@k: |relevant in top-k| / k.\n",
    "       If len(preds) < k, missing slots are treated as non-relevant (denominator still k).\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        return 0.0\n",
    "    topk = preds[:k]\n",
    "    hits = sum(1 for p in topk if p in gold_set)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k_single(preds, gold_set, k):\n",
    "    \"\"\"Recall@k: |relevant in top-k| / |gold_set|. If gold_set empty -> 0.0.\"\"\"\n",
    "    if len(gold_set) == 0:\n",
    "        return 0.0\n",
    "    topk = preds[:k]\n",
    "    hits = sum(1 for p in topk if p in gold_set)\n",
    "    return hits / len(gold_set)\n",
    "\n",
    "def average_precision(preds, gold_set):\n",
    "    \"\"\"AP: sum_{i:pred_i in gold} (precision@i) / |gold_set| ; returns 0 if gold_set empty.\"\"\"\n",
    "    if len(gold_set) == 0:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "    for i, p in enumerate(preds, start=1):\n",
    "        if p in gold_set:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / i\n",
    "    if hits == 0:\n",
    "        return 0.0\n",
    "    return sum_precisions / len(gold_set)\n",
    "\n",
    "def f_beta(prec, rec, beta=2.0):\n",
    "    if prec == 0 and rec == 0:\n",
    "        return 0.0\n",
    "    b2 = beta * beta\n",
    "    return (1 + b2) * (prec * rec) / (b2 * prec + rec)\n",
    "\n",
    "# -----------------------\n",
    "# Load gold\n",
    "with open(f\"./kg/data_parsed/{SELECTED_ID}_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    gold = json.load(f)\n",
    "\n",
    "# normalize gold ids to strings\n",
    "for qid, info in gold.items():\n",
    "    gold[qid][\"retrieved_list\"] = [str(x).strip() for x in info.get(\"retrieved_list\", [])]\n",
    "\n",
    "# -----------------------\n",
    "# Prepare preds df (uses in-memory step3_top2_df)\n",
    "preds_df = submission_df.copy()\n",
    "\n",
    "# Ensure preds are lists of strings\n",
    "def ensure_list_of_str(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str(v) for v in x]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            # crude parse: split on commas\n",
    "            items = [it.strip().strip(\"'\\\"\") for it in s[1:-1].split(\",\") if it.strip() != \"\"]\n",
    "            return [str(it) for it in items]\n",
    "        return [s]\n",
    "    # fallback\n",
    "    return [str(x)]\n",
    "\n",
    "preds_df[\"preds\"] = preds_df[\"article_id\"].apply(ensure_list_of_str)\n",
    "\n",
    "# Determine K for precision@k / recall@k\n",
    "if MAX_K is None:\n",
    "    max_pred_len = preds_df[\"preds\"].apply(len).max()\n",
    "    # but also consider gold length if you prefer; here we pick max prediction length\n",
    "    K = int(max(1, max_pred_len))\n",
    "else:\n",
    "    K = int(MAX_K)\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate per query\n",
    "rows = []\n",
    "# matrices for precision@k & recall@k\n",
    "prec_at_k_matrix = []  # list of lists per query\n",
    "rec_at_k_matrix = []\n",
    "\n",
    "for _, r in preds_df.iterrows():\n",
    "    qid = r[\"query_id\"]\n",
    "    preds = r[\"preds\"]\n",
    "    gold_list = gold.get(qid, {}).get(\"retrieved_list\", [])\n",
    "    gold_set = set(gold_list)\n",
    "\n",
    "    # overall precision using full predicted list (len(preds) as denom)\n",
    "    full_prec = (sum(1 for p in preds if p in gold_set) / len(preds)) if len(preds) > 0 else 0.0\n",
    "    full_rec = (sum(1 for p in preds if p in gold_set) / len(gold_set)) if len(gold_set) > 0 else 0.0\n",
    "\n",
    "    # F2 using full_prec, full_rec\n",
    "    f2 = f_beta(full_prec, full_rec, beta=2.0)\n",
    "\n",
    "    # AP\n",
    "    ap = average_precision(preds, gold_set)\n",
    "\n",
    "    # precision@k and recall@k for k=1..K\n",
    "    prec_k = [precision_at_k_single(preds, gold_set, k) for k in range(1, K+1)]\n",
    "    rec_k = [recall_at_k_single(preds, gold_set, k) for k in range(1, K+1)]\n",
    "\n",
    "    prec_at_k_matrix.append(prec_k)\n",
    "    rec_at_k_matrix.append(rec_k)\n",
    "\n",
    "    rows.append({\n",
    "        \"query_id\": qid,\n",
    "        \"n_pred\": len(preds),\n",
    "        \"n_gold\": len(gold_list),\n",
    "        \"precision_full\": full_prec,\n",
    "        \"recall_full\": full_rec,\n",
    "        \"f2_full\": f2,\n",
    "        \"AP\": ap\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------\n",
    "# Aggregate summary\n",
    "mean_precision_full = df_metrics[\"precision_full\"].mean()\n",
    "mean_recall_full = df_metrics[\"recall_full\"].mean()\n",
    "mean_f2_full = df_metrics[\"f2_full\"].mean()\n",
    "\n",
    "map_all = df_metrics[\"AP\"].mean()\n",
    "map_relevant = df_metrics.loc[df_metrics[\"n_gold\"]>0, \"AP\"].mean()\n",
    "\n",
    "# Precision@k and Recall@k averaged across queries (treating missing preds as non-relevant)\n",
    "prec_at_k_arr = np.mean(np.array(prec_at_k_matrix), axis=0) if len(prec_at_k_matrix)>0 else np.zeros(K)\n",
    "rec_at_k_arr = np.mean(np.array(rec_at_k_matrix), axis=0) if len(rec_at_k_matrix)>0 else np.zeros(K)\n",
    "\n",
    "# -----------------------\n",
    "# Print summary\n",
    "print(\"=== Aggregate retrieval metrics ===\")\n",
    "print(f\"Queries evaluated : {len(df_metrics)}\")\n",
    "print(f\"Mean Precision (full predicted lists) : {mean_precision_full:.4f}\")\n",
    "print(f\"Mean Recall (full predicted lists)    : {mean_recall_full:.4f}\")\n",
    "print(f\"Mean F2 (Î²=2)                         : {mean_f2_full:.4f}\")\n",
    "print(f\"MAP (all queries; AP=0 for empty)    : {map_all:.4f}\")\n",
    "print(f\"MAP (only queries with >=1 gold)     : {map_relevant:.4f}\")\n",
    "print()\n",
    "print(\"Precision@k (k=1..{}):\".format(K))\n",
    "for k, val in enumerate(prec_at_k_arr, start=1):\n",
    "    print(f\"  P@{k}: {val:.4f}\")\n",
    "print()\n",
    "print(\"Recall@k (k=1..{}):\".format(K))\n",
    "for k, val in enumerate(rec_at_k_arr, start=1):\n",
    "    print(f\"  R@{k}: {val:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Expose results for further inspection\n",
    "# df_metrics contains per-query metrics\n",
    "# prec_at_k_arr and rec_at_k_arr contain averaged P@k / R@k across queries\n",
    "# You can examine per-query AP distribution:\n",
    "df_metrics_sorted = df_metrics.sort_values(\"AP\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# show top/bottom problematic queries\n",
    "print(\"\\nTop 5 queries by AP:\")\n",
    "display(df_metrics_sorted.head(5))\n",
    "print(\"\\nBottom 5 queries by AP (including zero AP):\")\n",
    "display(df_metrics_sorted.tail(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
